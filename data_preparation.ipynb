{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, combinations\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS TO PREPARE THE DATASET\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def empty_fill(df):\n",
    "    taxons = ['Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species']\n",
    "    taxon_acronym = ['k_', 'p_', 'c_', 'o_', 'f_', 'g_', ' sp']\n",
    "    # enumerate over each column in the dataframe \n",
    "    # to automatically label the empty entries according to the aformentioned labelling\n",
    "    for index, taxon in enumerate(taxons):\n",
    "        # only at the species level, acryonym is added at the end\n",
    "        if taxon == taxons[-1]:\n",
    "            df[taxon][df[taxon].isna()] = df[taxons[index-1]] + taxon_acronym[index]\n",
    "        else:\n",
    "            df[taxon][df[taxon].isna()] = taxon_acronym[index] + df[taxons[index-1]]     \n",
    "    return df\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def standardize_sp(df):\n",
    "    not_sub_df = df[df['Species'].str.contains(' ')]\n",
    "    sub_df = df[ ~ df['Species'].str.contains(' ')]\n",
    "    sub_df_copy = sub_df.copy()\n",
    "    sub_df_copy['Species'] = sub_df['Genus'] + ' ' + sub_df['Species']\n",
    "    return pd.concat([not_sub_df, sub_df_copy])\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def keep_min_max_len(df, min_max = (1000, 2000)):\n",
    "    seq_len_series = df['Sequence'].apply(lambda seq: len(seq))\n",
    "    min_bool = min_max[0] <= seq_len_series\n",
    "    max_bool = seq_len_series <= min_max[1]\n",
    "    new_df = df[min_bool & max_bool]\n",
    "    return new_df\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def U2T(df):\n",
    "    new_df = df.copy()\n",
    "    new_df['Sequence'] = df['Sequence'].apply(lambda seq: seq.replace('U', 'T'))\n",
    "    return new_df\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def min_entries(df, taxon = 'Species', min = 10):\n",
    "    return df[df[taxon].map(df[taxon].value_counts()) >= min]\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "prim = 'GGCGGACGGGTGAGTAA'\n",
    "common_prim = [\n",
    "    'GGCGGACGGGTGAGTAA', 'GGCGCACGGGTGCGTAA', 'GGCGAACGGGCGAGTCA', 'GGCGAAAGGGTGAGTAA',\n",
    "    'GGCGACCGGGTGAGTAA', 'GGCGAACGGGTGCGTAA', 'GGCGGACGGGTGAGGAA', 'GCCAAACGGGGTAGTAA']\n",
    "possible_prim = []\n",
    "\n",
    "for indices in combinations(range(len(prim)), 14):\n",
    "    for letters in product('ATGC', repeat = 3):\n",
    "        letter_iter = iter(letters)\n",
    "        possible_prim.append(\n",
    "            ''.join([prim[x] if x in indices else letter_iter.__next__() for x in range(len(prim))]))\n",
    "\n",
    "random.shuffle(possible_prim)\n",
    "primers = common_prim + possible_prim\n",
    "\n",
    "def find_start(seq):\n",
    "    start_region = seq[:300]\n",
    "    for primer in possible_prim:\n",
    "        index = start_region.find(primer)\n",
    "        if index > -1:\n",
    "            return index\n",
    "    return index\n",
    "\n",
    "def line_up(df):\n",
    "    new_df = df.copy()\n",
    "    temp_df = df.copy()\n",
    "    indeces = df['Sequence'].copy().apply(\n",
    "        lambda seq: find_start(seq))\n",
    "    temp_df['Index'] = indeces\n",
    "    new_df['Sequence'] = temp_df.apply(\n",
    "        lambda x: x['Sequence'][x['Index']:] if x['Index'] != -1 else np.nan, axis=1)\n",
    "    return new_df.dropna()\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "complement_dict = {\n",
    "    'A': 'T', 'C': 'G', 'T': 'A', 'G': 'C', 'U':'A', \n",
    "    'Y':'R', 'R':'Y', 'W':'W', 'S':'S', 'K':'M', 'M':'K', \n",
    "    'D':'H', 'V':'B', 'H':'D', 'B':'V', 'N':'N', 'X':'X', '-':'-'}\n",
    "\n",
    "def reverse_complement(seq):\n",
    "    seq_list = list(seq)\n",
    "    seq_list = [complement_dict[base] for base in seq_list]\n",
    "    return ''.join(seq_list)[::-1]\n",
    "\n",
    "def rev_comp(df):\n",
    "    new_df = df.copy()\n",
    "    new_df['Sequence'] = df['Sequence'].apply(lambda seq : reverse_complement(seq))\n",
    "    return df\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def most_frequent(arr):\n",
    "    # returns the most frequent element in array\n",
    "    unique_nucl, counts = np.unique(arr, return_counts = True)\n",
    "    return unique_nucl[counts.argmax()]\n",
    "\n",
    "def consensus(str_lst):\n",
    "    i_short = len(min(str_lst, key=len))\n",
    "    # generate 2D array with nucleotides\n",
    "    cons_lst = np.column_stack(([list(seq[:i_short]) for seq in str_lst]))\n",
    "    return ''.join(np.apply_along_axis(most_frequent, 1, cons_lst))\n",
    "\n",
    "def normalize(df, taxon = 'Species'):\n",
    "    old_df = df.sample(frac = 1).copy()\n",
    "    new_df = pd.DataFrame({\n",
    "        'Kingdom':[], 'Phylum':[], 'Class':[], 'Order':[], 'Family':[], 'Genus':[], 'Species':[], 'Sequence':[]})\n",
    "    # values and arrays used for distinguishing and choosing over/under-represented labels\n",
    "    tax_count = old_df[taxon].value_counts()\n",
    "    tax_avg = tax_count.mean()\n",
    "    tax_std = statistics.stdev(tax_count)\n",
    "    # over/under-representation is checked for each unique label\n",
    "    unique_labels = old_df[taxon].unique()\n",
    "    for label in unique_labels:\n",
    "        label_count = tax_count[label]\n",
    "        entries = old_df.loc[old_df[taxon] == label].copy()\n",
    "        groups = round(tax_avg + tax_std + (label_count // tax_avg))\n",
    "        if label_count > groups:\n",
    "            new_entries = entries[:groups].copy()\n",
    "            cons_seqs = []\n",
    "            start = 0\n",
    "            stop = label_count // groups\n",
    "            for group in range(groups):\n",
    "                sequences = entries[start:stop]['Sequence'].tolist()\n",
    "                cons_seqs.append(consensus(sequences))\n",
    "                start = stop\n",
    "                stop += label_count // groups\n",
    "            new_entries['Sequence'] = cons_seqs\n",
    "            new_df = pd.concat([new_df, new_entries])\n",
    "        else:\n",
    "            new_df = pd.concat([new_df, entries])  \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset standardized (69675, 8)\n",
      "1st restriction (49961, 8)\n",
      "sequences lined up & de-duplicated (40561, 8)\n",
      "dataset normalized (30547, 8)\n",
      "total dataset with augmentation (61094, 8)\n",
      "dataset restricted (25446, 8) (59728, 8)\n",
      "dataset saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "# Loading the dataset as a pandas data frame ---------------------------------------------------\n",
    "df = pd.read_csv('data/full_curated_dataset.csv')\n",
    "\n",
    "# Labeling empty cells with an informative value -----------------------------------------------\n",
    "df0 = empty_fill(df)\n",
    "\n",
    "# Standardizing the species columns ------------------------------------------------------------\n",
    "df1 = standardize_sp(df0)\n",
    "\n",
    "# Standardizing the sequence length and T/U ----------------------------------------------------\n",
    "df2 = keep_min_max_len(df1, min_max = (1300, 1600))\n",
    "df2 = U2T(df2)\n",
    "print('dataset standardized', df2.shape)\n",
    "\n",
    "# Initial restriction to reduce memory requirements --------------------------------------------\n",
    "df3 = min_entries(df2, taxon = 'Species', min = 3)\n",
    "print('1st restriction', df3.shape)\n",
    "\n",
    "# Line up sequences ----------------------------------------------------------------------------\n",
    "df4 = line_up(df3).drop_duplicates(subset = ['Species', 'Sequence'])\n",
    "print('sequences lined up & de-duplicated', df4.shape)\n",
    "\n",
    "# Normalize dataset by collapsing sequences of overrepresented species -------------------------\n",
    "df5 = normalize(df4)\n",
    "print('dataset normalized', df5.shape)\n",
    "\n",
    "# Increase the amount of entries by adding the reverse complement sequences --------------------\n",
    "rc_df = rev_comp(df5)\n",
    "df6 = pd.concat([df5, rc_df], ignore_index = True)\n",
    "print('total dataset with augmentation', df6.shape)\n",
    "\n",
    "# Removal of too underrepresented species ------------------------------------------------------\n",
    "df7 = min_entries(df5, taxon = 'Species', min = 5).sort_values(by = ['Family', 'Genus', 'Species'])\n",
    "df7RC = min_entries(df6, taxon = 'Species', min = 5).sort_values(by = ['Family', 'Genus', 'Species'])\n",
    "print('dataset restricted', df7.shape, df7RC.shape)\n",
    "\n",
    "# Save train/val datasets ----------------------------------------------------------------------\n",
    "df7.to_csv('df_noRC.csv', index = False)\n",
    "df7RC.to_csv('df_wRC.csv', index = False)\n",
    "print('dataset saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461\n",
      "961\n",
      "342\n"
     ]
    }
   ],
   "source": [
    "print(df7['Species'].nunique())\n",
    "print(df7['Genus'].nunique())\n",
    "print(df7['Family'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2801\n",
      "1472\n",
      "417\n"
     ]
    }
   ],
   "source": [
    "print(df7RC['Species'].nunique())\n",
    "print(df7RC['Genus'].nunique())\n",
    "print(df7RC['Family'].nunique())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e755af18eee0034cd700295d0e984a40f53659443b610e850ee570bdbea72f7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
