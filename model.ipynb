{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded!\n"
     ]
    }
   ],
   "source": [
    "# LOADING PACKAGES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from varname import argname\n",
    "import time\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, accuracy_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, Masking, Dot, Add, BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "LR, BATCH_SIZE, EPOCHS, MAX_LEN = 0.001, 128, 50, 1600\n",
    "INPUT_SHAPE_1HOT, INPUT_SHAPE_2MER, INPUT_SHAPE_4MER, INPUT_SHAPE_6MER = (1600, 4), (16, 2), (256, 2), (1600, 2)\n",
    "\n",
    "print('Packages loaded!')\n",
    "\n",
    "# AMOUNT OF UNIQUE LABELS AT EACH TAXON LEVEL\n",
    "fam_count, gen_count, spe_count = 349, 954, 1569"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq2tax(out_len, name = 'seq2tax'):\n",
    "    input_1HOT = Input(shape = INPUT_SHAPE_1HOT)\n",
    "    input_2MER = Input(shape = INPUT_SHAPE_2MER)\n",
    "    input_4MER = Input(shape = INPUT_SHAPE_4MER)\n",
    "    input_6MER = Input(shape = INPUT_SHAPE_6MER)\n",
    "\n",
    "\n",
    "    ## CONV Layers\n",
    "    X_cnn = X_mask\n",
    "    # conv_net\n",
    "    for i in range(CONV_NET_nr):\n",
    "        X_cnn = conv_net_block(X_cnn, n_cnn_filters=NET_filters, cnn_window=NET_window)\n",
    "    # res_net\n",
    "    for i in range(RES_NET_nr):\n",
    "        X_cnn = res_net_block(X_cnn, n_cnn_filters=NET_filters, cnn_window=NET_window)\n",
    "\n",
    "    ## Extra Pooling layer and Dropout\n",
    "    X_pool = AveragePooling1D(pool_size=POOL_s)(X_cnn)\n",
    "    X_drop = Dropout(DROP_r)(X_pool)\n",
    "\n",
    "    ## RNN Layers\n",
    "    H_lstm = Bidirectional(LSTM(LSTM_nodes, return_sequences=True), merge_mode='sum')(X_drop)\n",
    "    H_lstm = Activation('tanh')(H_lstm)\n",
    "\n",
    "    ## ATT Layers\n",
    "    r_emb = attention_layer(H_lstm, n_layer=ATT_layers, n_node=ATT_nodes, block_name = 'att')\n",
    "        \n",
    "    # Fully connected layers\n",
    "    r_emb = fully_connected(r_emb, n_layer=FC_layers, n_node=FC_nodes, drop_out_rate=FC_drop, block_name = 'fc')\n",
    "\n",
    "    # Compile model\n",
    "    out_Kin = Dense(kin_count, activation='softmax', name='Kingdom_out')(r_emb)\n",
    "    out_Phy = Dense(phy_count, activation='softmax', name='Phylum_out')(r_emb)\n",
    "    out_Cla = Dense(cla_count, activation='softmax', name='Class_out')(r_emb)\n",
    "    out_Ord = Dense(ord_count, activation='softmax', name='Order_out')(r_emb)\n",
    "    out_Fam = Dense(fam_count, activation='softmax', name='Family_out')(r_emb)\n",
    "    out_Gen = Dense(gen_count, activation='softmax', name='Genus_out')(r_emb)\n",
    "    out_Spe = Dense(spe_count, activation='softmax', name='Species_out')(r_emb)\n",
    "\n",
    "    R2Pmodel = Model(inputs = X, outputs = out, name = name)\n",
    "    \n",
    "    return R2Pmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa = ['Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read2Pheno\n",
    "## Conv & Res net layers\n",
    "CONV_NET_nr, RES_NET_nr, NET_filters, NET_window = 2, 1, 64, 2\n",
    "## extra Dropout layer (after Res block)\n",
    "DROP_r, POOL_s = 0.2, 2\n",
    "## BiLSTM layer\n",
    "LSTM_nodes = 128\n",
    "## attention Layers\n",
    "ATT_layers, ATT_nodes = 1, 128\n",
    "## fully connected layers\n",
    "FC_layers, FC_nodes, FC_drop = 1, 128, 0.3\n",
    "\n",
    "# BLOCK FUNCTIONS\n",
    "def conv_net_block(X, n_cnn_filters=256, cnn_window=9, block_name='convblock'):\n",
    "    '''\n",
    "    convolutional block with a 1D convolutional layer, a batch norm layer followed by a relu activation.\n",
    "    parameters:\n",
    "        n_cnn_filters: number of output channels\n",
    "        cnn_window: window size of the 1D convolutional layer\n",
    "    '''\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "def res_net_block(X, n_cnn_filters=256, cnn_window=9, block_name='resblock'):\n",
    "    '''\n",
    "    residual net block accomplished by a few convolutional blocks.\n",
    "    parameters:\n",
    "        n_cnn_filters: number of output channels\n",
    "        cnn_window: window size of the 1D convolutional layer\n",
    "    '''\n",
    "    X_identity = X\n",
    "    # cnn0\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    # cnn1\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    # cnn2\n",
    "    X = Conv1D(n_cnn_filters, cnn_window, strides=1, padding='same')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Add()([X, X_identity])\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "def attention_layer(H_lstm, n_layer, n_node, block_name='att'):\n",
    "    '''\n",
    "    feedforward attention layer accomplished by time distributed dense layers.\n",
    "    parameters:\n",
    "        n_layer: number of hidden layers\n",
    "        n_node: number of hidden nodes\n",
    "    '''\n",
    "    H_emb = H_lstm\n",
    "    for i in range(n_layer):\n",
    "        H_lstm = TimeDistributed(Dense(n_node, activation=\"tanh\"))(H_lstm)\n",
    "    M = TimeDistributed(Dense(1, activation=\"linear\"))(H_lstm)\n",
    "    alpha = keras.layers.Softmax(axis=1)(M)\n",
    "    r_emb = Dot(axes = 1)([alpha, H_emb])\n",
    "    r_emb = Flatten()(r_emb)\n",
    "    return r_emb\n",
    "\n",
    "def fully_connected(r_emb, n_layer, n_node, drop_out_rate=0.5, block_name='fc'):\n",
    "    '''\n",
    "    fully_connected layer consists of a few dense layers.\n",
    "    parameters:\n",
    "        n_layer: number of hidden layers\n",
    "        n_node: number of hidden nodes\n",
    "        drop_out_rate: dropout rate to prevent the model from overfitting\n",
    "    '''\n",
    "    for i in range(n_layer):\n",
    "        r_emb = Dense(n_node, activation=\"relu\")(r_emb)\n",
    "    r_emb = Dropout(drop_out_rate)(r_emb) \n",
    "    return r_emb\n",
    "\n",
    "# TOTAL MODEL FUNCTION\n",
    "def make_R2Pmodel(out_len, INPUT_SHAPE=INPUT_SHAPE_RNN, name='Read2Pheno'):\n",
    "    X = Input(shape=INPUT_SHAPE)\n",
    "    X_mask = Masking(mask_value=0.)(X)\n",
    "\n",
    "    ## CONV Layers\n",
    "    X_cnn = X_mask\n",
    "    # conv_net\n",
    "    for i in range(CONV_NET_nr):\n",
    "        X_cnn = conv_net_block(X_cnn, n_cnn_filters=NET_filters, cnn_window=NET_window)\n",
    "    # res_net\n",
    "    for i in range(RES_NET_nr):\n",
    "        X_cnn = res_net_block(X_cnn, n_cnn_filters=NET_filters, cnn_window=NET_window)\n",
    "\n",
    "    ## Extra Pooling layer and Dropout\n",
    "    X_pool = AveragePooling1D(pool_size=POOL_s)(X_cnn)\n",
    "    X_drop = Dropout(DROP_r)(X_pool)\n",
    "\n",
    "    ## RNN Layers\n",
    "    H_lstm = Bidirectional(LSTM(LSTM_nodes, return_sequences=True), merge_mode='sum')(X_drop)\n",
    "    H_lstm = Activation('tanh')(H_lstm)\n",
    "\n",
    "    ## ATT Layers\n",
    "    r_emb = attention_layer(H_lstm, n_layer=ATT_layers, n_node=ATT_nodes, block_name = 'att')\n",
    "        \n",
    "    # Fully connected layers\n",
    "    r_emb = fully_connected(r_emb, n_layer=FC_layers, n_node=FC_nodes, drop_out_rate=FC_drop, block_name = 'fc')\n",
    "\n",
    "    # Compile model\n",
    "    out = Dense(out_len, activation='softmax', name='final_dense')(r_emb)\n",
    "    R2Pmodel = Model(inputs = X, outputs = out, name = name)\n",
    "    \n",
    "    return R2Pmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_data, train_labels, validation_data, validation_labels, test_data, test_labels):\n",
    "    wandb.init(project = 'Final Training', entity = 'bachelorprojectgroup9', name=model.name)\n",
    "\n",
    "    print (f'Loading {model.name} model...')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=LR), metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    print (f'Fitting {model.name} model...')\n",
    "    start_time = time.time()\n",
    "    history = model.fit(train_data, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data = (validation_data, validation_labels), callbacks=[WandbCallback()])\n",
    "    time_taken = round(time.time() - start_time)\n",
    "    # history object is saved and can later be destinguished using the model/train_data names\n",
    "    np.save('history/{}_{}.npy'.format(model.name, argname('train_data')), history.history)\n",
    "    \n",
    "    print (f'Evaluating {model.name} model...')\n",
    "    test_labels_arg = np.argmax(test_labels, axis=1)\n",
    "    test_predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "    loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "\n",
    "    # F1-score: harmonic mean of the precision and recall\n",
    "    #   score from 0 to 1\n",
    "    f1 = f1_score(y_true=test_labels_arg, y_pred=test_predictions, average='weighted')\n",
    "    # Matthews correlation coefficient: coefficient of +1 represents a perfect prediction,\n",
    "    #   0 an average random prediction and -1 an inverse prediction\n",
    "    mcc = matthews_corrcoef(y_true=test_labels_arg, y_pred=test_predictions)\n",
    "\n",
    "    score_dict = pd.DataFrame({'Model/run' : model.name, 'Data' : argname('train_data'), 'Training time' : time_taken, 'Test loss' : loss, 'Test accuracy' : accuracy, 'F1-score' : f1, 'MCC' : mcc}, index=[0])\n",
    "    print(score_dict)\n",
    "    # score metrics are saved and can later be destinguished using the model names\n",
    "    score_dict.to_csv(f'scores/{model.name}_evaluation.csv', index=False)\n",
    "\n",
    "    wandb.finish()\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e755af18eee0034cd700295d0e984a40f53659443b610e850ee570bdbea72f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
